{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "#### Authors:\n",
    "v1.0 (Spring 2020) Justin Hong, Christina Zhang, Kannan Ramchandran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Introduction: a continuous example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have a security camera that outputs a signal — some number $X$. We know that when all is well, the signal will be distributed as standard normal: $X \\sim N(0, 1)$. But if there is an intruder, then the distribution of the signal is shifted, and $X \\sim N(2, 1)$. When we read the signal from the camera, we would like to determine if all is well, or if there is an intruder. If there is an intruder, we would like to correctly determine so. At the same time, we would like to avoid rushing home if there was no intruder after all. \n",
    "\n",
    "This is an example of a binary hypothesis testing problem. There are two possible hypotheses. The first is that all is well. We can call this $H_0$, the **null hypothesis**. After observing $X$, we can **accept** the null hypothesis, or we can **reject** the null hypothesis in favor of the other hypothesis, $H_1$ — that there is an intruder.\n",
    "\n",
    "We can write a rule for accepting or rejecting the null hypothesis by determining an **acceptance region**. This will be all values of $X$ for which we accept the null hypothesis. An arbitrary example of this could be saying if $X \\in \\{3\\} \\bigcup [6, 9]$, accept that there is no intruder. \n",
    "\n",
    "After we've set an acceptance region, it is possible to raise a false alarm (incorrectly reject the null hypothesis). For example, it is possible that $X \\sim N(0, 1)$, meaning all is well, and $X = 4$, which is outside of the acceptance region. In this case our rule would say that there is an intruder when there is not. The conditional probability of making this type of error is called the **probability of false alarm** (PFA) and is denoted $\\alpha$. \n",
    "\n",
    "$$\\text{PFA} = \\alpha = \\mathcal{P}(\\text{choosing } H_1 | H_0)$$\n",
    "\n",
    "On the other hand, it could be that there is an intruder, and we detect it correctly (for example,  $X \\sim N(2, 1)$ and $X = 4$). The probability of this is the **probability of correct detection** (PCD).\n",
    "\n",
    "$$\\text{PCD} = \\mathcal{P}(\\text{choosing } H_1 | H_1)$$\n",
    "\n",
    "We would like to maximize PCD while staying under a limit on PFA. We'll see that there is a tradeoff between these two values. \n",
    "\n",
    "For now, let's take a look at the PDFs of $X$ in each case: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "import scipy.stats\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0 = 0\n",
    "mu1 = 2\n",
    "variance = 1\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(1 - 5*sigma, 1 + 5*sigma, 100)\n",
    "y0 = scipy.stats.norm.pdf(x, mu0, sigma)\n",
    "y1 = scipy.stats.norm.pdf(x, mu1, sigma)\n",
    "fig, (ax1) = plt.subplots(1, 1, sharex=True)\n",
    "\n",
    "ax1.set_title('Possible PDFs of X')\n",
    "ax1.plot(x, y0, color='red', label='H0')\n",
    "ax1.plot(x, y1, color='blue', label='H1')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('P(X = x)')\n",
    "ax1.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the graph above, we have some feeling that if we observe $X < 1$, $H_0$ is more likely, and if we observe $X > 1$, $H_1$ is more likely.\n",
    "\n",
    "Why is this? You may be comparing the conditional probabilities $f_{X|H_1}(x)$ and $f_{X|H_0}(x)$. When $X < 1$, $f_{X|H_0}(x) > f_{X|H_1}(x)$, and when $X > 1$, $f_{X|H_0}(x) < f_{X|H_1}(x)$.\n",
    "Define the **likelihood ratio** as the ratio of these two values. \n",
    "\n",
    "$$ L(x) := \\frac{f_{H_1}(x)}{f_{H_0}(x)} $$\n",
    "\n",
    "Plot the likelihood ratio as a function of $x$ (Note: if plt is not working, try stopping the interactive plots from before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN YOUR SOLUTION\n",
    "\n",
    "# END YOUR SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the likelihood ratio is mononotonically increasing. Neyman-Pearson says that when this is the case, the acceptance region is something like this: \n",
    "\n",
    "1. Accept $H_0$ if $X < c$\n",
    "\n",
    "2. Reject $H_0$ if $X > c$\n",
    "\n",
    "(Because $X$ is continuous, we don't consider $X = c$ here. But that case is important for discrete RVs, as we will see in the next example.)\n",
    "\n",
    "Because the likelihood ratio is monotonically increasing, we can be confident that there's not some high value of $X$, say $c'>c$, beyond which we would change our minds and accept $H_0$ again. In other words, we know that there exists some threshold $c$ for $X$ below which we always accept, and above which we always reject the null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neyman-Pearson Test \n",
    "\n",
    "Let's visualize the acceptance and rejection regions derived from the Neyman-Pearson test. When our likelihood ratio is monotonic, we know the 'optimal' test is a simple threshold decision rule. \n",
    "\n",
    "Calculate the threshold and the PCD for the hypotheses described above. (Hint: Look up norm.cdf and norm.ppf from scipy.stats)\n",
    "\n",
    "Play around with the value of $\\alpha$ and look at the visualization. The x values of the shaded region indicate what values of $X$ we should reject the null hypothesis. The area of the shaded region is the probability of false alarm. How are PFA and PCD related to each other? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Returns the NP threshold for the continuous example above for a given significance level alpha.\n",
    "def get_threshold(alpha):\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION \n",
    "    return threshold\n",
    "\n",
    "# Returns the probability of correct detection for the continuous example above\n",
    "# given a threshold (for a decision rule).\n",
    "def get_pcd(threshold):\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION \n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0 = 0\n",
    "mu1 = 2\n",
    "variance = 1\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(1 - 5*sigma, 1 + 5*sigma, 100)\n",
    "y0 = scipy.stats.norm.pdf(x, mu0, sigma)\n",
    "y1 = scipy.stats.norm.pdf(x, mu1, sigma)\n",
    "\n",
    "fig, (ax1) = plt.subplots(1)\n",
    "\n",
    "ax1.set_title('Neyman-Pearson')\n",
    "ax1.plot(x, y0, color='red', label='H0')\n",
    "ax1.plot(x, y1, color='blue', label='H1')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.xaxis.set_label_coords(0.5, -0.02)\n",
    "ax1.set_ylabel('P(X = x)')\n",
    "pcd_text = plt.text(-4, 0.37, 'PCD: ')\n",
    "pfa_text = plt.text(-4, 0.34, 'PFA: ')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "@interact(alpha=0.0)\n",
    "def update_figure(alpha):\n",
    "    threshold = get_threshold(alpha)                            \n",
    "    ax1.fill_between(x, 0, y0, where=(True), color='white', alpha=1)\n",
    "    ax1.fill_between(x, 0, y1, where=(True), color='white', alpha=1)\n",
    "    ax1.fill_between(x, 0, y0, where=(x>threshold), color='red', alpha=0.3)\n",
    "    ax1.fill_between(x, 0, y1, where=(x>threshold), color='blue', alpha=0.3)\n",
    "    pcd = get_pcd(threshold)\n",
    "    pcd_text.set_text('PCD: {}'.format(np.around(pcd, decimals=4)))\n",
    "    pfa_text.set_text('PFA: {}'.format(np.around(alpha, decimals=4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "Play around with the significance level (alpha) slider. How does PFA change with significance level? How does PCD change with significance level?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "How would the acceptance region look if we had a monotonically decreasing likelihood ratio rather than a monotonically increasing one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A discrete example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's consider a discrete example. \n",
    "\n",
    "We observe $X \\sim Bin(n,p)$, and wish to test $H_0: p=p_0$ vs $H_1: p=p_1$, with $p_0 < p_1$.\n",
    "\n",
    "Implement the Neyman-Pearson test for discrete distributions. Run the cells to see a discrete visualization. Note: when the bars are partially filled in the visualization, this indicates we randomize our decision when we observe that value of $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants.\n",
    "n = 10\n",
    "p0 = 0.4\n",
    "p1 = 0.6\n",
    "\n",
    "bar_width=0.3\n",
    "display_num_decimals=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs0 = scipy.stats.binom.pmf(np.arange(n+1), n, p0)\n",
    "probs1 = scipy.stats.binom.pmf(np.arange(n+1), n, p1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1, 1, sharex=True)\n",
    "ax1.set_title('Likelihood ratio')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.scatter(range(11), [probs1[i]/probs0[i] for i in range(11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_threshold_test_dist(alpha, probs0):\n",
    "    \"\"\"Returns a function which, given input data x, returns the\n",
    "    rejection probability for the level alpha simple threshold test\n",
    "    based on the probability distribution described by probs0.\n",
    "    E.g. If you deterministically reject for a given observation x,\n",
    "    test(x) should output 1. \"\"\"\n",
    "\n",
    "    # CCDF w/ equality (Defined as CCDF_w_eq(x) = P(X >= x)) under the null hypothesis.\n",
    "    ccdf_w_eq = np.cumsum(probs0[::-1])[::-1]\n",
    "    # For numerical imprecision issues.\n",
    "    ccdf_w_eq[0] = 1.0\n",
    "    # Find the first index from the right where the tail probability is >= alpha.\n",
    "    threshold = len(ccdf_w_eq) - 1 - np.argmax(ccdf_w_eq[::-1] >= alpha)\n",
    "    \n",
    "    def test(x):\n",
    "        ### BEGIN SOLUTION\n",
    "        \n",
    "        ### END SOLUTION\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bars for binomial probabilities.\n",
    "fig, (ax1) = plt.subplots(1)\n",
    "\n",
    "background_bars0 = ax1.bar(np.arange(n+1) - bar_width / 2, probs0, \n",
    "                           bar_width, color= '#FFE9E9', edgecolor = 'black')\n",
    "background_bars1 = ax1.bar(np.arange(n+1) + bar_width / 2, probs1, \n",
    "                           bar_width, color= '#E9FFFE', edgecolor = 'black')\n",
    "bars0 = ax1.bar(np.arange(n+1) - bar_width / 2, 0.0, bar_width, color = 'red',\n",
    "                label='H0: X~Bin(n, 0.4)')\n",
    "bars1 = ax1.bar(np.arange(n+1) + bar_width / 2, 0.0, bar_width, color = 'blue', linestyle='-',\n",
    "                label='H1: X~Bin(n, 0.6)')\n",
    "ax1.set_xticks(np.arange(11))\n",
    "height = max(max(probs0), max(probs1))\n",
    "pcd_text = ax1.text(-0.3, height * 0.95, 'PCD: ')\n",
    "pfa_text = ax1.text(-0.3, height * 0.9, 'PFA: ')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('x')\n",
    "ax1.xaxis.set_label_coords(0.45, -0.02)\n",
    "ax1.set_ylabel('P(X = x)')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "@interact(alpha=0.0)\n",
    "def update_figure(alpha):\n",
    "    test = simple_threshold_test_dist(alpha, probs0)\n",
    "    \n",
    "    rejection_probs = [test(i) for i in range(n+1)]\n",
    "    # List of PCD, given each possible data value.\n",
    "    pcd = [probs1[i] * rejection_probs[i] for i in range(n+1)]\n",
    "    # List of PFA, given each possible data value.\n",
    "    pfa = [probs0[i] * rejection_probs[i] for i in range(n+1)]\n",
    "    \n",
    "    # Updates total PCD and PFA.\n",
    "    pcd_text.set_text('PCD: {}'.format(np.around(sum(pcd), decimals=4)))\n",
    "    pfa_text.set_text('PFA: {}'.format(np.around(sum(pfa), decimals=4)))\n",
    "    \n",
    "    # Updates the bars.\n",
    "    for i in range(n+1):\n",
    "        bars0[i].set_height(rejection_probs[i] * probs0[i])\n",
    "        bars1[i].set_height(rejection_probs[i] * probs1[i])\n",
    "    fig.canvas.draw_idle()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "What is the probability that $X = 6$ given $H_0$? $H_1$? Compare $\\mathcal{P}{(X=6|H_1)}$ and $\\mathcal{P}{(X=6|H_0)}$. In what case would we choose to accept the hypothesis corresponding to the lesser of these values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "Play around with the significance level (alpha) slider. What is the best PCD value you can find for $\\alpha \\le 0.10$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Suppose we want the probability of false alarm to be no more than 0.1, and we observe $X$ to be 5. Which parameter, $p_0$ or $p_1$, would we ascribe to our model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Note that when the significance level is 0.1, and we observe $X$ to be 6, whether we reject the null hypothesis is not determinate. Suppose we eschew this and decide to set the probability to either 0 or 1. In each case, what do we violate about the Neyman Pearson guarantees?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Consider the distribution assumed by $H_0$. Notice that under $H_0$, the probability that $X=10$ is extremely small. Why do we accept the null hypothesis when the significance level is 0? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Deriving the optimal acceptance region \n",
    "\n",
    "Recall the likelihood ratio: probability of the observation given H1 / probability of the observation given H0. Let us forget about Neyman-Pearson for a moment. Intuitively, we should reject the null hypothesis when the likelihood ratio is large, and accept it when it is small. Part of the formulation of the hypothesis testing problem is that we want to maximize the probability of correct detection, i.e. reject the null hypothesis when it is false. However, we cannot always reject the null hypothesis, because rejecting the null hypothesis when it is correct is a false alarm. \n",
    "\n",
    "Write code that will greedily choose where to reject based on the highest likelihood ratios, so that the PCD is maximized while PFA $\\leq \\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = list(range(11))\n",
    "H1_probs = probs1 # H1_probs[i] = Probability under H1 [X = i]\n",
    "H0_probs = probs0 # H0_probs[i] = Probability under H0 [X = i]\n",
    "\n",
    "# Find the likelihood ratio for each outcome and sort the outcomes by decreasing likelihood ratio \n",
    "def get_sorted_outcomes(H1_probs, H0_probs):\n",
    "    sorted_outcomes = None\n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "    \n",
    "    ### END YOUR SOLUTION ###\n",
    "    return sorted_outcomes\n",
    "\n",
    "sorted_outcomes = get_sorted_outcomes(H1_probs, H0_probs)\n",
    "print(sorted_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return rej_probs, where rej_probs[i] is the probability we reject the null when we observe i as the outcome\n",
    "# under a Neyman-Pearson rule with significance level alpha\n",
    "def get_rejection_probabilities(alpha, sorted_outcomes, H0_probs):\n",
    "    # Find the rejection rule \n",
    "    rej_probs = [0]*11\n",
    "    pfa_so_far = 0\n",
    "    \n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "    ### END YOUR SOLUTION ###\n",
    "    return rej_probs\n",
    "\n",
    "alpha = 0.10\n",
    "rej_probs = get_rejection_probabilities(alpha, sorted_outcomes, H0_probs)\n",
    "print(rej_probs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the probability of correct detection for the Neyman-Pearson rule with significance level alpha\n",
    "def get_pcd_greedy(rej_probs, H1_probs):\n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "    ### END YOUR SOLUTION ###\n",
    "print(get_pcd_greedy(rej_probs, H1_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does your greedy algorithm's result compare with the likelihood ratio test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bimodal example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a case where the simple threshold test is not optimal.\n",
    "\n",
    "In this case, $X$ will be one of two mixtures of binomials. See the likelihood ratio and PDFs plotted below. Note that in the visualization, we reject from right to left, but this is not the best test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bimodal \n",
    "helper0 = scipy.stats.binom.pmf(np.arange(n+1), n, .1)\n",
    "helper1 = scipy.stats.binom.pmf(np.arange(n+1), n, .7) \n",
    "helper2 = scipy.stats.binom.pmf(np.arange(n+1), n, .3)\n",
    "helper3 = scipy.stats.binom.pmf(np.arange(n+1), n, .9) \n",
    "probsbi0 = (helper0 + helper1)/2\n",
    "probsbi1 = (helper2 + helper3)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1, 1, sharex=True)\n",
    "ax1.set_title('Likelihood ratio')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.scatter(range(11), [probsbi1[i]/probsbi0[i] for i in range(11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax2) = plt.subplots(1)\n",
    "background_bars0 = ax2.bar(np.arange(n+1) - bar_width / 2, probsbi0, \n",
    "                           bar_width, color= '#FFE9E9', edgecolor = 'black')\n",
    "background_bars1 = ax2.bar(np.arange(n+1) + bar_width / 2, probsbi1, \n",
    "                           bar_width, color= '#E9FFFE', edgecolor = 'black')\n",
    "bars0 = ax2.bar(np.arange(n+1) - bar_width / 2, 0.0, bar_width, color = 'red',\n",
    "                label='H0')\n",
    "bars1 = ax2.bar(np.arange(n+1) + bar_width / 2, 0.0, bar_width, color = 'blue', linestyle='-',\n",
    "                label='H1')\n",
    "ax2.set_xticks(np.arange(11))\n",
    "height = max(max(probsbi0), max(probsbi1))\n",
    "pcd_text = ax2.text(2, height * 0.95, 'PCD: ')\n",
    "pfa_text = ax2.text(2, height * 0.9, 'PFA: ')\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('x')\n",
    "ax2.xaxis.set_label_coords(0.45, -0.02)\n",
    "ax2.set_ylabel('P(X = x)')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "@interact(alpha=0.0)\n",
    "def update_figure(alpha):\n",
    "    test = simple_threshold_test_dist(alpha, probsbi0)\n",
    "    \n",
    "    rejection_probs = [test(i) for i in range(n+1)]\n",
    "    \n",
    "    # List of PCD, given each possible data value.\n",
    "    pcd = [probsbi1[i] * rejection_probs[i] for i in range(n+1)]\n",
    "    # List of PFA, given each possible data value.\n",
    "    pfa = [probsbi0[i] * rejection_probs[i] for i in range(n+1)]\n",
    "    \n",
    "    # Updates total PCD and PFA.\n",
    "    pcd_text.set_text('PCD: {}'.format(np.around(sum(pcd), decimals=2)))\n",
    "    pfa_text.set_text('PFA: {}'.format(np.around(sum(pfa), decimals=2)))\n",
    "\n",
    "    # Updates the bars.\n",
    "    for i in range(n+1):\n",
    "        bars0[i].set_height(rejection_probs[i] * probsbi0[i])\n",
    "        bars1[i].set_height(rejection_probs[i] * probsbi1[i])\n",
    "    fig.canvas.draw_idle()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next three cells which use your greedy algorithm to determine the acceptance and rejection regions. How do the rejection probabilities and PCD compare to accepting from right to left? What makes this example different from the last example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_outcomes = get_sorted_outcomes(probsbi1, probsbi0)\n",
    "print(sorted_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "rej_probs = get_rejection_probabilities(alpha, sorted_outcomes, probsbi0)\n",
    "print(rej_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the PCD \n",
    "print(get_pcd_greedy(rej_probs, probsbi1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Using the slider above, what is the best PCD you can find for PFA $\\le \\alpha$ assuming a threshold decision rule?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question:\n",
    "Compute the PCD using the greedy strategy. How does this compare to the the threshold rule?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Q3. Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, we have learned so far how to create a 95% confidence interval for the Gaussian distribution given an observation or a batch of observations. However, for other distributions, the answer is unclear. In this section, we explore the method of 'inverting' a hypothesis test to find a confidence interval for an arbitrary distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition, a 95% confidence interval is an interval for which the true parameter, $\\theta$, is contained within the interval with probability 95%. Suppose for some fixed $\\theta_0 \\in \\Theta$ we construct a hypothesis test, $H_0: \\theta = \\theta_0$ and $H_1: \\theta \\neq \\theta_0$. If we find the set, \n",
    "\n",
    "$$C(X) = \\{ \\theta_0: \\text{the test for }H_0: \\theta = \\theta_0 \\text{ is accepted under PFA constraint }\\alpha \\}$$\n",
    "\n",
    "then we will have found a $(1-\\alpha)$-level confidence interval/set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have mostly covered one-sided hypothesis tests using Neyman-Pearson. For the sake of reproducing results you have seen before, you may assume that a symmetrical two-tailed decision rule (e.g. For $X\\sim N(0,1)$, accept for $X\\in [-0.5, 0.5]$) is the optimal for the symmetrical distributions we will be working with (Gaussian and Binomial with $p=1/2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the confidence interval for a Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X\\sim N(\\theta, 1)$ and we observe $X = 2$. Let's first implement the hypothesis test for $X$ given some fixed $\\theta_0$ and an observation $X=x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a tuple containing the decision of the hypothesis test and the p-value associated with the observation.\n",
    "def gaussian_hypo_test(theta0, x, alpha):\n",
    "    r, p_val = 0, 0\n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "    ### END YOUR SOLUTION ###\n",
    "    return r, p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check: should return (0, 1)\n",
    "gaussian_hypo_test(2, 2, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To invert the test, we need to run this hypothesis test on all possible values of $\\theta_0 \\in \\Theta$, which is impossible since $\\Theta = \\mathbb{R}$. One method is to take an initial guess, then take discrete steps until we find the whole interval. Let us try to implement that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try intializing with an underestimate, ensuring that we begin with a rejection (outside of the interval)\n",
    "# then stepping in the positive direction.\n",
    "def compute_gaussian_CI(x, alpha, init_guess, step=0.01):\n",
    "    assert alpha != 0 and init_guess < x and gaussian_hypo_test(init_guess, x, alpha)[0] == 1, \"Must start with an underestimate\"\n",
    "    conf_interval_start = None\n",
    "    conf_interval_end = None\n",
    "    theta0 = init_guess\n",
    "    \n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "    ### END YOUR SOLUTION ###\n",
    "    \n",
    "    conf_interval = (conf_interval_start, conf_interval_end)\n",
    "    return conf_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"95% confidence interval for theta is: {}\".format(compute_gaussian_CI(2, 0.05, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for this example, the confidence interval is a contiguous interval where the boundaries are where the p-value meets $\\alpha$ with equality. We can use this to find the confidence interval more accurately and much faster. (Hint: use the inverse CDF of the Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gaussian_CI_alt(x, alpha):\n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "    ### END YOUR SOLUTION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_gaussian_CI_alt(2, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    " Compute the 95% confidence interval for $\\theta$ and compare it to the confidence intervals you computed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the confidence interval for a Binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try an example where we have not learned any analytical tools to help us compute the confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X \\sim Binomial(n, 1/2)$, and we observe $X=15$. What is a 95% confidence interval for $n$? Start by implementing a hypothesis test for the Binomial given a fixed value $n_0 \\in \\{1, 2, \\dots\\}$ and observation $x$. For the sake of simplicity, ignore randomization in the test (where randomization is necessary just accept the null)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial_hypo_test(n0, x, alpha):\n",
    "    r, p_val = 0, 0\n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "    ### END YOUR SOLUTION ###\n",
    "    return r, p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check: both should return (0, 1) or something close\n",
    "print(binomial_hypo_test(30, 15, 0.05))\n",
    "print(binomial_hypo_test(29, 15, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our test, we can invert the test similar to how we did the Gaussian. Only this time, a safe initial guess is 0, and we should increment by 1 since $n_0$ must be discrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binomial_CI(x, alpha, init_guess=0, step=1):\n",
    "    # Note, no assertion on the initial guess since 0 may be in the interval for non trivial alpha\n",
    "    conf_interval_start = None\n",
    "    conf_interval_end = None\n",
    "    n0 = init_guess\n",
    "    \n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "    ### END YOUR SOLUTION ###\n",
    "    \n",
    "    conf_interval = (conf_interval_start, conf_interval_end)\n",
    "    return conf_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"95% confidence interval for n is: {}\".format(compute_binomial_CI(15, 0.05)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different values of $\\alpha$ and see how it affects the width of the interval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
