{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Filtering\n",
    "\n",
    "v1.0 (2020 Spring): Aditya Sengupta, William Gan, Kannan Ramchandran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kalman_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, we'll cover applications of the Kalman filter, which is the optimal linear algorithm for sensor fusion: combining information from sensors and from an underlying physics model of a system to get the best possible state estimate.\n",
    "\n",
    "At its core, the Kalman filter is optimal because it incorporates covariance matrices. As an example, suppose I’m trying to estimate a random variable $X$, and my current estimate is $\\hat{X} = 10$. I have another random variable $Y = X + W$, where $W$ is some noise, and upon observing $Y$, I see that it’s 15. How should I update $\\hat{X}$ then? Should it be 12.5? Well, if $X$ has very low variance, then maybe it should be closer to 10. If $W$ has very large variance, then an observation of 15 might not be too crazy, so it should be closer to 10 as well. In the opposite situations, we could see why it might be closer to 15. Based on the covariance matrices of $X$ and $W$, we can figure out what optimizes minimum mean square error.\n",
    "\n",
    "In more detail, say our current estimate of a random variable $X$ is $\\hat{X}$ and we have Gaussian covariance $\\Sigma_1$ around it. Suppose we then observe $Y \\sim \\mathcal{N}(X, \\Sigma_2)$, and want to update our estimate; this comes out to \n",
    "\n",
    "$$\\hat{X}_{\\text{updated}} = \\hat{X} + \\Sigma_1(\\Sigma_1 + \\Sigma_2)^{-1}(Y - \\hat{X}) = \\hat{X} + K(Y - \\hat{X})$$\n",
    "$$\\Sigma_{\\text{updated}} = \\Sigma_1 - \\Sigma_1(\\Sigma_1 + \\Sigma_2)^{-1}\\Sigma_1 = (I - K)\\Sigma_1$$\n",
    "\n",
    "These are essentially taking the first Gaussian, and adjusting it so that it ends up somewhere in between the first and the second one. We can pull out a common \"adjustment factor\", $K = \\Sigma_1(\\Sigma_1 + \\Sigma_2)^{-1}$, and call it the Kalman gain. The estimate increases with a factor proportional to the Kalman gain, and the covariance reduces by a factor proportional to the Kalman gain.\n",
    "\n",
    "The rest of the Kalman filter math just deals with how to set up these two Gaussians, which will turn out to only require a few matrix multiplications!\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "This lab needs the Python package `pyautogui`, which provides functions to control your computer's mouse and keyboard. If you don't already have it, you can pip install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pyautogui -q\n",
    "import pyautogui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Visualizing the Filter's Action\n",
    "\n",
    "Before we implement the filter, let's visualize how it reduces state error. Suppose we can describe a system's state in a vector $\\vec{x} = \\begin{matrix} [x_1 & x_2] \\end{matrix}^\\intercal$. At a certain time, we can make a prediction of that state based on the last state, and then we can measure the state to see how far off that prediction is. Neither of these estimates are perfect, but by somehow averaging them, we can get the best fused estimate!\n",
    "\n",
    "### Question 1a: Plotting Different Averages\n",
    "In the following cells, implement two different ways of averaging $\\hat{x}$ and $y$.\n",
    "1. The unweighted average of the two data points, in `unweighted_average`\n",
    "2. The weighted average considering the full covariance elements, in `overlap_mean_and_covs`\n",
    "\n",
    "The cell after that plots three ways of overlapping the estimates: the unweighted average, the weighted average with only diagonal variances, and the full weighted average. You should be able to see the 1 SD ellipse around the estimate (in purple) shrinking as we incorporate more and more information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unweighted_average(x_hat, y):\n",
    "    ### BEGIN YOUR SOLUTION\n",
    "    \n",
    "    ### END YOUR SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_mean_and_covs(x_hat, sigma_1, y, sigma_2):\n",
    "    ### BEGIN YOUR SOLUTION\n",
    "    x_hat_updated = \n",
    "    sigma_updated = \n",
    "    ### END YOUR SOLUTION\n",
    "    return x_hat_updated, sigma_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array([3, 5])\n",
    "measurement = np.array([2, 7])\n",
    "prediction_cov = np.array([[4, 1], [1, 0.81]])\n",
    "measurement_cov = np.array([[0.25, -1.44], [-1.44, 9]])\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].set_xlim([0, 5])\n",
    "    axs[i].set_ylim([2, 10])\n",
    "    axs[i].scatter(*prediction, color='blue')\n",
    "    axs[i].scatter(*measurement, color='red')\n",
    "\n",
    "axs[0].scatter(*unweighted_average(prediction, measurement), color='purple')\n",
    "\n",
    "plot_one_sd_blob(prediction, np.diag(np.diag(prediction_cov)), axs[1], edgecolor='blue')\n",
    "plot_one_sd_blob(measurement, np.diag(np.diag(measurement_cov)), axs[1], edgecolor='red')\n",
    "mean_b, cov_b = overlap_mean_and_covs(prediction, np.diag(np.diag(prediction_cov)), measurement, np.diag(np.diag(measurement_cov)))\n",
    "axs[1].scatter(*mean_b, color='purple')\n",
    "plot_one_sd_blob(mean_b, cov_b, axs[1], edgecolor='purple')\n",
    "\n",
    "plot_one_sd_blob(prediction, prediction_cov, axs[2], edgecolor='blue')\n",
    "plot_one_sd_blob(measurement, measurement_cov, axs[2], edgecolor='red')\n",
    "mean_c, cov_c = overlap_mean_and_covs(prediction, prediction_cov, measurement, measurement_cov)\n",
    "axs[2].scatter(*mean_c, color='purple')\n",
    "plot_one_sd_blob(mean_c, cov_c, axs[2], edgecolor='purple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with the means/covariances (although if you set the covariance to be something that isn't positive semidefinite, it'll try to take the square root of a negative number and the results won't be valid).\n",
    "\n",
    "### Question 1b: Comparing Averages\n",
    "\n",
    "**Where does the original, unweighted average of the two points lie in relation to the overlap? Is it a good estimate of the weighted average?**\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Implementation and Kinematics\n",
    "\n",
    "Let's implement a generic Kalman filter, and then apply it to some simple dynamic models!\n",
    "\n",
    "The predict/update equations are reproduced here from the note on the website, with the equation numbers from that note:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\hat{X}_{n + 1 \\mid n} = A \\hat{X}_{n \\mid n} & \\text{(eq 6)}\\\\\n",
    "\\Sigma_{n + 1 \\mid n} = A\\Sigma_{n \\mid n} A^\\intercal + \\Sigma_V & \\text{(eq 11)}\\\\\n",
    "K_n = \\Sigma_{n + 1 \\mid n} C^\\intercal (C \\Sigma_{n + 1 \\mid n} C^\\intercal + \\Sigma_W)^{-1} & \\text{(eq 10)}\\\\\n",
    "\\hat{X}_{n + 1 \\mid n + 1} = \\hat{X}_{n + 1 \\mid n} + K(Y_n - C\\hat{X}_{n + 1 \\mid n}) & \\text{(eq 8)} \n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "### Question 2a: Implementation\n",
    "\n",
    "**Fill in the code to complete the ```predict``` and ```update``` functions below.** It may be helpful to refer to the vector case Kalman filter equations from the notes. A few clarifications:\n",
    "\n",
    "- `self.state` is our current estimate $\\hat{X}_n$.\n",
    "- `self.P` is the covariance around the current estimate $\\Sigma_{n \\mid n}$.\n",
    "- `self.Q` is $\\Sigma_V$, `self.R` is $\\Sigma_W$.\n",
    "- In `predict`, change `self.state` to $\\hat{X}_{n + 1 \\mid n}$ and `self.P` to $\\Sigma_{n+1 \\mid n}$.\n",
    "- In `update`, calculate `self.K` (if not in steady state), and change `self.P` to $\\Sigma_{n+1 \\mid n+1}$. Also update `self.state` to $\\hat{X}_{n+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFilter(AbstractKFilter):\n",
    "    def __init__(self, A, Q, C, R, state=None):\n",
    "        self.A = A\n",
    "        self.Q = Q\n",
    "        self.C = C\n",
    "        self.R = R\n",
    "        self.s = A.shape[0]\n",
    "        self.m = C.shape[0]\n",
    "        if state is None:\n",
    "            self.state = np.zeros(self.s)\n",
    "        else:\n",
    "            self.state = state\n",
    "        self.prev_P = np.zeros((self.s, self.s))\n",
    "        self.P = np.zeros((self.s, self.s))\n",
    "        self.steady_state = False\n",
    "    \n",
    "    def predict(self):\n",
    "        self.prev_P = copy.deepcopy(self.P)\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        self.state =\n",
    "        self.P =\n",
    "        ### END YOUR SOLUTION\n",
    "        \n",
    "    def update(self, measurement):\n",
    "        if not self.steady_state:\n",
    "            ### BEGIN YOUR SOLUTION\n",
    "            self.K = \n",
    "            self.P =\n",
    "            ### END YOUR SOLUTION\n",
    "            if np.allclose(self.P, self.prev_P):\n",
    "                self.steady_state = True\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        innovation = \n",
    "        self.state =\n",
    "        ### END YOUR SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the following test to verify your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict_and_update():\n",
    "    A = np.array([[2, 1], [1, 2]])\n",
    "    Q = np.array([[1, 0], [1, 1]])\n",
    "    C = np.array([[1, 1]])\n",
    "    R = np.array([[0.5]])\n",
    "    state = np.array([3, 1])\n",
    "    kf = KFilter(A, Q, C, R, state)\n",
    "    kf.predict()\n",
    "    assert np.allclose(kf.prev_P, np.array([[0, 0], [0, 0]]))\n",
    "    assert np.allclose(kf.state, np.array([7, 5]))\n",
    "    assert np.allclose(kf.P, np.array([[1, 0], [1, 1]]))\n",
    "    kf.update(np.array([11]))\n",
    "    assert np.allclose(kf.K, np.array([[2/7], [4/7]]))\n",
    "    assert np.allclose(kf.P, np.array([[3/7, -2/7], [-1/7, 3/7]]))\n",
    "    assert np.allclose(kf.state, np.array([47/7, 31/7]))\n",
    "    print('All assertions passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_and_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try some simple examples! To start with, let's consider some object that we start at rest, and that moves under some unknown random forces. We won't deal with modelling these forces yet; we'll just consider how they impact position and velocity over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 1e-3\n",
    "kinematics_data = pd.read_csv('kinematics.csv', header=0) \n",
    "times = kinematics_data['t']\n",
    "measurements_k = kinematics_data['measurements']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(times, measurements_k, color='red')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Measurements')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the measurements are too noisy for us to have an accurate idea of where the object is over time, but since we know the laws of motion it follows, we can improve that estimate a lot!\n",
    "\n",
    "### Question 2b: Linear Tracking\n",
    "**Based on the following information, define the matrices A, Q, C, R below and run the filter to generate MMSE estimates of position and velocity.**\n",
    "1. The object moves according to $p[n+1] = p[n] + v[n] \\Delta t$, where $\\Delta t = 10^{-3}$s (this was defined above in the variable `dt`).\n",
    "2. The velocity is expected to stay constant, but may be perturbed by a force that causes a $\\mathcal{N}(0, 1)$ change.\n",
    "3. Measurements are only of position, and have a *standard deviation* of 1.5.\n",
    "\n",
    "Once these have been defined, the line `states_2b = kinematics_forward.simulate(measurements_k)` will iterate through the measurements, compute the predictions and state updates, and return an array of size (state_size, num_timesteps) containing the state at each timestep. (The number of timesteps is set by the number of measurements; here it's 1000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN YOUR SOLUTION\n",
    "A = ...\n",
    "Q = ...\n",
    "C = ...\n",
    "R = ...\n",
    "### END YOUR SOLUTION\n",
    "kinematics_forward = KFilter(A, Q, C, R)\n",
    "states_2b = kinematics_forward.simulate(measurements_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "axs[0].plot(times, kinematics_data[\"truth_p\"], label='Truth positions')\n",
    "axs[0].plot(times, ..., label='Filtered positions') # YOUR ESTIMATE HERE\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel('Time')\n",
    "axs[0].set_ylabel('Position')\n",
    "axs[1].plot(times, measurements_k, label='Measurements')\n",
    "axs[1].plot(times, ..., label='Filtered positions') # YOUR ESTIMATE HERE\n",
    "axs[1].legend()\n",
    "axs[1].set_xlabel('Time')\n",
    "axs[1].set_ylabel('Position')\n",
    "pos_error_measurements = percent_rms_err(kinematics_data['truth_p'], measurements_k)\n",
    "pos_error_predictions = percent_rms_err(kinematics_data['truth_p'], ...) # YOUR ESTIMATE HERE\n",
    "print(f'Percent error of measurements from truth: {pos_error_measurements:.2f}')\n",
    "print(f'Percent error of KFilter predictions from truth: {pos_error_predictions:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we can do is look at velocity! If you try to numerically differentiate velocity (take pointwise differences and divide them by the time interval), it's really sensitive to little perturbations and won't have much coherence over time. If you run the cell below, you'll see that the measurement of velocity (differences in position between steps, divided by the time-step) is off by orders of magnitude, and also has high variance: it's essentially white noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differentiated_positions = np.insert(np.diff(measurements_k), 0, 0) / dt\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "axs[0].plot(times, differentiated_positions, color='red')\n",
    "axs[0].set_title('Differentiated positions')\n",
    "axs[0].set_xlabel('Time')\n",
    "axs[0].set_ylabel('Velocity')\n",
    "axs[1].plot(times, kinematics_data['truth_v'], color='blue')\n",
    "axs[1].set_title('Truth velocity')\n",
    "axs[1].set_xlabel('Time')\n",
    "axs[1].set_ylabel('Velocity')\n",
    "vel_error_diff = percent_rms_err(differentiated_positions, kinematics_data['truth_v'])\n",
    "print(f'Percent error of differentiated positions from truth: {vel_error_diff:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To contrast that, let's plot the Kalman filtered velocity, which gives us the MMSE of velocity over time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(times, kinematics_data['truth_v'], label='Truth velocity')\n",
    "plt.plot(times, ..., label='Kalman filtered velocity') # YOUR ESTIMATE HERE\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Velocity\")\n",
    "vel_error_predictions = percent_rms_err(states_2b[1], kinematics_data['truth_v'])\n",
    "print(f'Percent error of KFilter predictions from truth: {vel_error_predictions:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still don't have any direct measurements of velocity, but we've been able to leverage what we know about the state and measurement variances to get a much better estimate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2c: Kalman Smoothing\n",
    "\n",
    "In real-time control situations, the above is the best estimate we're able to get. However, when we're postprocessing, we can also run the filter *backwards* and average the two results to smooth out the estimates.\n",
    "\n",
    "**(i)** Modify any of the input matrices as needed to create a filter that runs backwards in time through all the measurements, and produce state estimates from that. Then, average the two runs and compare the RMS errors in position and velocity to those from the forward run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN YOUR SOLUTION\n",
    "A_back = A\n",
    "Q_back = Q\n",
    "C_back = C\n",
    "R_back = R\n",
    "### END YOUR SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinematics_backward = KFilter(A_back, Q_back, C_back, R_back, states_2b.T[-1])\n",
    "reversed_measurements = kinematics_data['measurements'][::-1]\n",
    "states_2c = np.flip(kinematics_backward.simulate(reversed_measurements), axis=1)\n",
    "smooth_states = (states_2b + states_2c) / 2\n",
    "pos_error_smooth = percent_rms_err(..., kinematics_data[\"truth_p\"]) # YOUR POS ESTIMATE HERE\n",
    "vel_error_smooth = percent_rms_err(..., kinematics_data[\"truth_v\"]) # YOUR VEL ESTIMATE HERE\n",
    "print(f'Old Position Error: {pos_error_predictions:.2f} New Error: {pos_error_smooth:.2f}')\n",
    "print(f'Old Velocity Error: {vel_error_predictions:.2f} New Error: {vel_error_smooth:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(ii)** Look at the covariance matrices for the forward and backward directions. What differences do you see between them? Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinematics_forward.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinematics_backward.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2d: Mouse Movement\n",
    "\n",
    "We can easily extend our one-dimensional tracker to two dimensions; by doing this, we've developed a filter that'll accurately return states of, for example, a robot moving around on a flat plane. In this question, we'll explore how to tune and interpret such a model, using the movement of your mouse across your computer screen as measurements! \n",
    "\n",
    "In the cell below, define your own A, Q, C, R matrices by making a model that you think accurately represents moving your mouse across the screen (with an added measurement noise term, to more closely model a real-life system). Then run the cell, move your mouse around the screen, and see how well your filter does! Play around with the $Q$ and $R$ values, or with the way you move your mouse, and note how performance changes. Try and get the RMS error below the baseline (by more than a pixel). \n",
    "\n",
    "General comments to help you do this:\n",
    "- $x$ and $y$ motion can be modeled independently of one another, both similarly to how you did it in 2b (you may also make a more complicated model here and see how it affects the error).\n",
    "- $Q$ and $R$ can be diagonal matrices in almost every use case, and the diagonal values can be tuned based on what works. How much do you think is reasonable for process error in this case? Measurement error?\n",
    "- It's also okay to change $dt$, though that shouldn't be necessary.\n",
    "\n",
    "Once you've done this, briefly answer the following questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 100\n",
    "measurement_error_sd = 20 * min(pyautogui.size()) / 1000\n",
    "dt = 1e-1\n",
    "A = # YOUR CODE HERE\n",
    "Q = # YOUR CODE HERE\n",
    "C = # YOUR CODE HERE\n",
    "R = # YOUR CODE HERE\n",
    "\n",
    "tracker = KFilter(A, Q, C, R)\n",
    "predictions = np.zeros((num_iters, 2))\n",
    "updates = np.zeros((num_iters, 2))\n",
    "truths = np.zeros((num_iters, 2))\n",
    "\n",
    "print(\"Start moving!\")\n",
    "time.sleep(1)\n",
    "iters = 0\n",
    "total_error = 0\n",
    "\n",
    "duration = dt\n",
    "while iters < num_iters:\n",
    "    t = time.time() * 1e6\n",
    "    tracker.predict()\n",
    "    prediction = tracker.measure()\n",
    "    predictions[iters] = prediction\n",
    "    m = np.array(pyautogui.position() + np.random.normal(0, measurement_error_sd, (2,)))\n",
    "    truths[iters] = np.array(pyautogui.position())\n",
    "    tracker.update(m)\n",
    "    updated = tracker.measure()\n",
    "    updates[iters] = updated\n",
    "    if iters > 0: # skip the first iteration because the initial state is likely far off\n",
    "        total_error += np.sum((updated - truths[iters]) ** 2)\n",
    "    iters += 1\n",
    "    duration = (time.time() * 1e6 - t) / 1e6\n",
    "    time.sleep(max(0, dt - duration))\n",
    "\n",
    "print(\"\\nBaseline         \", np.sqrt(2) * measurement_error_sd, \"px\")\n",
    "print(\"Tracker RMS Error\", np.sqrt(total_error / iters), \"px\")\n",
    "screensize = pyautogui.size()\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axs[0].set_xlim((0, screensize[0]))\n",
    "axs[0].set_ylim((screensize[1], 0))\n",
    "axs[0].set_xlabel(r\"$x$ (px)\")\n",
    "axs[0].set_ylabel(r\"$y$ (px)\")\n",
    "axs[0].plot(truths[:,0], truths[:,1], 'ko-', label='Truth')\n",
    "axs[0].plot(updates[:,0], updates[:,1], 'ro-', label='Filtered')\n",
    "axs[0].set_title(\"Tracking\")\n",
    "axs[0].legend()\n",
    "axs[1].set_xlim((-screensize[0]/2, screensize[0]/2))\n",
    "axs[1].set_ylim((-screensize[1]/2, screensize[1]/2))\n",
    "axs[1].set_xlabel(r\"$\\Delta x$ (px)\")\n",
    "axs[1].set_ylabel(r\"$\\Delta y$ (px)\")\n",
    "axs[1].scatter(*(updates.T - truths.T))\n",
    "axs[1].set_title(\"Residuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i) Briefly describe how you defined $A$, $Q$, $C$, and $R$, and report the approximate tracker RMS error.**\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "**(ii) While varying the model parameters, you may have seen the tracker doing about the same as the baseline (if you didn't, try setting $Q$ really low or really high.) What might cause this?**\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Sinusoidal Motion and System Identification\n",
    "\n",
    "So far we've only discussed linear motion, but it's possible to build models (choices of $A, Q, C, R$) that let us track a signal that's the solution to any ordinary differential equation. Partial differential equations are possible too, but more complicated.\n",
    "\n",
    "In particular, we'll deal with _autoregressive_ tracking of a sinusoidal signal, i.e. one of the form $x(t) = A \\sin(2\\pi f t + \\phi)$. In this case, $A, f, \\phi$ are all unknowns, but we'll see that all we need to identify is $f$.\n",
    "\n",
    "Instead of a position/velocity state like we used in question 2, here we'll use the state $\\vec{x}[n] = \\begin{bmatrix} p[n] & p[n-1] \\end{bmatrix}^\\intercal$: the state is the combination of the current position and the last one. Further, we can find the update rule by solving a boundary value problem:\n",
    "\n",
    "$$p[n] = 2\\cos(2\\pi f \\Delta t) p[n-1] - p[n-2]$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Identification\n",
    "\n",
    "Identifying the frequency is an example of a *system identification* step: something we need to do before we start tracking so that we can build the best possible model. In this example, time-domain methods aren't the most efficient for finding frequencies, so instead we'll do a Fourier transform and choose the peak.\n",
    "\n",
    "The cell below generates a sinusoidal signal with position and velocity variances given by `q_pos` and `q_vel`, but with hidden amplitude, frequency, and phase. It then computes a Fourier transform of the measured positions and finds the location of the peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 1e-2\n",
    "times = np.arange(0, 1000 * dt, dt)\n",
    "sinusoid = make_sinusoid(times, dt, q_pos=0.05, q_vel=0.05)\n",
    "plt.plot(times, sinusoid)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Position (arbitrary units)\")\n",
    "plt.title(\"Sinusoidal motion\")\n",
    "plt.show()\n",
    "\n",
    "measurements = sinusoid + np.random.normal(0, 1, times.size)\n",
    "\n",
    "# we don't know the frequency, so let's find it with Fourier analysis\n",
    "freqs = np.fft.fftfreq(measurements.size, d=dt)\n",
    "transformed = np.abs(np.fft.fft(measurements))\n",
    "plt.plot(freqs, transformed)\n",
    "plt.xlabel(\"Frequencies (Hz)\")\n",
    "plt.ylabel(\"Signal power\")\n",
    "plt.title(\"Fourier transform of sinusoidal signal\")\n",
    "\n",
    "f = freqs[np.argmax(transformed[1:transformed.size // 2]) + 1]\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're cheating a little bit here: by doing this analysis to find $f$, we're assuming we've already seen all the measurements instead of getting them in real time. In practice, it's usually okay to assume that the frequency won't change much over time, so you can measure for a few seconds to find $f$ and then use it in the filter, maybe continuing to update as you go. For this lab, we won't worry about that.\n",
    "\n",
    "### Question 3a: Autoregressive Sinusoid Tracker\n",
    "\n",
    "**Fill in the code defining the sine-tracking filter below, and run the filter on the measurements.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = # YOUR CODE HERE\n",
    "Q = # YOUR CODE HERE\n",
    "C = # YOUR CODE HERE\n",
    "R = # YOUR CODE HERE\n",
    "sinetracker = KFilter(A, Q, C, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = sinetracker.simulate(measurements)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(times, sinusoid, label=\"True process\")\n",
    "plt.plot(times, states[0], label=\"Kalman filtered\")\n",
    "plt.plot(times, states[0] - sinusoid, label=\"Error\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sine_measure_err = rms(sinusoid - measurements)\n",
    "sinetrack_err = rms(sinusoid[50:] - states[0][50:]) # average only after initial convergence\n",
    "\n",
    "print(f'Absolute error of measurements from truth: {sine_measure_err:.2f}')\n",
    "print(f'Absolute error of KFilter predictions from truth: {sinetrack_err:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3b: System Misidentification\n",
    "\n",
    "**Suppose we misidentified the frequency, and so our $A$ is wrong. Is it still possible to get decent state estimates? Experiment in the cells below, and answer below that.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_freq = 1.5 * f\n",
    "A[0][0] += 2 * np.sin(np.pi * (f + bad_freq) * dt) * np.sin(np.pi * (f - bad_freq) * dt) # trig magic\n",
    "# change any of these as needed\n",
    "### BEGIN YOUR CODE\n",
    "Q = Q\n",
    "C = C\n",
    "R = R\n",
    "### END YOUR CODE\n",
    "bad_sinetracker = KFilter(A, Q, C, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_states = bad_sinetracker.simulate(measurements)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(times, sinusoid, label=\"True process\")\n",
    "plt.plot(times, bad_states[0], label=\"Kalman filtered\")\n",
    "plt.plot(times, bad_states[0] - sinusoid, label=\"Error\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3c: Generating a Sine Wave\n",
    "\n",
    "The autoregressive model is just one way to track a sinusoidal signal, or one that's the solution to a differential equation: here, we'll briefly investigate another.\n",
    "\n",
    "Suppose we have a signal that can be expressed as the solution to a differential equation\n",
    "\n",
    "$$a_0 x(t) + a_1 x^{(1)}(t) + a_2 x^{(2)}(t) + \\dots + a_n x^{(n)}(t) = 0$$\n",
    "\n",
    "where the $x^{(i)}$s denote the $i$th derivatives. The following function generates a valid choice of the $A$ and $C$ matrices we need for the Kalman filter in the _controller canonical form_ (for what this means and how this works, see a control theory class like EE C128):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_from_DE(coeffs, dt=1e-3):\n",
    "    # coeffs : list, coeffs[i] = a_i\n",
    "    A, C, dt = cont2discrete(tf2ss([1], coeffs), dt)[::2]\n",
    "    transform = np.fliplr(np.identity(A.shape[0]))\n",
    "    return transform.dot(A).dot(transform), C.dot(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To test this, fill in the following line of code to generate a sine wave (which obeys $\\ddot{x} = -(2\\pi)^2 f^2 x$) that makes 4 oscillations in 10 seconds (1000 timesteps), by repeatedly applying the $A$ and $C$ matrices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1000\n",
    "dt = 1e-2\n",
    "DE_coeff = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, C = model_from_DE(DE_coeff, dt)\n",
    "times = np.arange(0, num_steps * dt, dt)\n",
    "state = np.array([0, 1])\n",
    "obs = np.zeros(1000,)\n",
    "for i in range(1000):\n",
    "    state = A @ state\n",
    "    obs[i] = C @ state\n",
    "plt.plot(times, obs)\n",
    "plt.title('Sine wave generated from state-space model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3d: Applying the Controller Canonical Model\n",
    "\n",
    "**If you go back and use this model in the above example, you'll find that tracking is, at best, not much better than the autoregressive case (maybe a 10% reduction in error at most). Why might this happen?**\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this lab, we've mostly focused on one aspect of Kalman filtering, namely postprocessing noisy measurements. This was so that we could illustrate the filter without the complexities of real-time control, which is the main topic of several other classes. However, the filter has many other applications and extensions. Some possible fun reads if you really love Kalman filtering now:\n",
    "\n",
    "1. [Kalman filtering for improving the precision of GPS](https://conference.scipy.org/proceedings/scipy2018/pdfs/mark_wickert_247.pdf)\n",
    "2. [A Kalman filter that learns its own transition model online](https://www.sciencedirect.com/science/article/abs/pii/S0262885612001783)\n",
    "3. [The Extended Kalman filter (EKF), which linearizes nonlinear systems so that we can still use the Kalman filter on them](https://www.cse.sc.edu/~terejanu/files/tutorialEKF.pdf)\n",
    "4. [The Ensemble Kalman filter (EnKF), which uses Monte Carlo methods to avoid doing derivatives in EKF problems](ftp://ftp.esat.kuleuven.be/pub/stadius/gillijns/reports/TR-05-58.pdf)\n",
    "5. [A Kalman filter in the frequency domain](https://arxiv.org/pdf/1808.08442.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources/References**\n",
    "\n",
    "1. https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/\n",
    "2. https://matplotlib.org/3.1.1/gallery/statistics/confidence_ellipse.html#sphx-glr-gallery-statistics-confidence-ellipse-py\n",
    "3. https://stackoverflow.com/questions/3129322/how-do-i-get-monitor-resolution-in-python/14124257\n",
    "4. https://www.cs.utexas.edu/~teammco/misc/kalman_filter/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
