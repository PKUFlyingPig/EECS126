{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Monte Carlo and Applications\n",
    "\n",
    "v1.0 (2018 Spring) Tavor Baharav, Kaylee Burns, Gary Cheng, Sinho Chewi, Hemang Jangle, William Gan, Alvin Kao, Chen Meng, Vrettos Muolos, Kanaad Parvate, Ray Ramamurti, Kannan Ramchandran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Markov Chain Monte Carlo methods are a powerful collection of techniques that allow us to sample from a distribution _even if we can't calculate the distribution directly._ This is useful for complex models, whose distributions may be intractable to compute. The idea is that, if we are able to sample from our desired distribution, we can answer any questions we may have about that distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is MCMC?\n",
    "\n",
    "Our goal is to simulate a Markov chain with a state for each outcome in our probability space. If the stationary distribution of the chain matches the distribution we want to sample from, then a random walk on the chain should perform like a sequence of samples from our desired distribution.\n",
    "\n",
    "In this lab we will be focusing on the Metropolis-Hastings algorithm, but this is not the only kind of MCMC method. Gibbs sampling, which you may have encountered in CS 188, is also a MCMC method. Gibbs sampling, however, requires computing a conditional distribution for each random variable in your model, which can be impractical and inefficient for some problems.\n",
    "\n",
    "We'll also explore an application of our algorithm to a sneaky spy challenge: use Metropolis-Hastings to decode the secret messages Will is sending to Justin! ðŸ•µï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Metropolis Hastings (MH)\n",
    "\n",
    "Our task is to define a set of transition and acceptance probabilities so that we have an aperiodic Markov Chain whose stationary distribution $\\pi(x)$ is equal to our target distribution $P(x)$. The MH algorithm does this implicitly by defining a procedure for transitioning between states. However, it has some assumptions. In particular, it assumes that you can compute:\n",
    "\n",
    "- $f(x)$, **a directly proportional estimate** of $P(x)$, i.e. $P(x) = \\frac{f(x)}{\\sum_{y \\in \\mathcal{X}} f(y)}$.\n",
    "- $g(x, \\cdot)$, a proposal distribution for the next state, where $x$ is your current state.\n",
    "\n",
    "Note: $g(x, \\cdot)$ can be arbitrary in terms of the $f(x)$ (e.g. N(0,1)) and achieve the same stationary distribution, but generally proposal functions have to be tuned on a per-case basis in order to visualize a good result in a reasonable amount of time).\n",
    "\n",
    "The MH algorithm says, at each time step:\n",
    "\n",
    "- Propose the next candidate state $y$ according $g(x,\\cdot)$.\n",
    "- Accept $y$, with probability $A(x,y) = min\\{1, \\frac{f(y)g(y,x)}{f(x)g(x,y)}\\}$.\n",
    "- If you accept the proposal, transition to $y$. Otherwise, stay in $x$.\n",
    "\n",
    "Following this procedure, the stationary distribution of the implicitly defined Markov Chain will be $P(x)$. Thus you can take a random walk to sample from $P(x)$. However, in practice, the following two extensions are made:\n",
    "\n",
    "- Taking every step-th state in the random walk. This helps reduce the dependence between samples.\n",
    "- Letting the chain walk a bit so that the distribution can converge to the stationary distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit of setup\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import utils\n",
    "\n",
    "from super_sneaky_secret import the_secret_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "In the cell below, implement Metropolis-Hastings according to the doc string. It should work for generic proposal, acceptance, and intitialization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings(proposal_func, init_func, score_func,\n",
    "                        num_iters, step=30):\n",
    "    \"\"\"\n",
    "    Runs the metropolis-hastings algorithm for\n",
    "    num_iters iterations, using proposal_func\n",
    "    to generate samples and scorer to assign\n",
    "    probability scores to samples.\n",
    "      \n",
    "    proposal_func -- function that proposes\n",
    "        candidate state; takes in current state as\n",
    "        argument and returns candidate state\n",
    "    init_func -- function that proposes starting\n",
    "        state; takes no arguments and returns a\n",
    "        sample state\n",
    "    score_func -- function that calculates f(y)/f(x)\n",
    "        * g(y,x)/g(x,y); takes in two state samples\n",
    "        (the current sample x then the candidate y).\n",
    "    \n",
    "    Returns a sequence of every step-th sample. You \n",
    "    should only sample on upon acceptance of a new\n",
    "    proposal. Do not keep sampling the current state.\n",
    "    \n",
    "    Note the total number of samples will NOT be\n",
    "    equal to num_iters. num_iters is the total number\n",
    "    of proposals we generate.\n",
    "    \"\"\"\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from Distributions Using MH\n",
    "\n",
    "Now that we have a method for sampling from distributions, let's apply it to some models. We'll start with very simple models so that we can compare the results from sampling with what we can compute analytically. This is also a useful opportunity for you to debug your implementation. Your implementation should be able to model a Gaussian and exponential distribution successfully.\n",
    "\n",
    "*Note: Our proposal distribution will be a normal distribution centered around our current state for the purposes of this lab. Take this into account when analyzing these examples.*\n",
    "\n",
    "*An interesting fact to note is that the algorithm works even for these **continuous distributions**. In this case the underlying Markov Chain will have a continuous state space ($\\mathbb{R}$)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Friendly Gaussian: $\\mathcal{N}(60, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prior = lambda: np.random.normal(loc=60)\n",
    "sample_candidate = lambda theta: np.random.normal(loc=theta)\n",
    "scorer = lambda x, y: (math.exp(-((y - 60)**2)/2)) / (math.exp(-((x - 60)**2)/2))\n",
    "normal_samples = metropolis_hastings(sample_candidate, sample_prior, scorer, 10000, 30)\n",
    "utils.plot_histogram_and_transitions(normal_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Distribution: $\\mathcal{Exp}(.5)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_pdf = utils.exp(0.5)\n",
    "exp_scorer = lambda x,y: exp_pdf(y) / exp_pdf(x)\n",
    "exp_prior = lambda : 10\n",
    "exp_samples = metropolis_hastings(sample_candidate, exp_prior, exp_scorer,10000)\n",
    "utils.plot_histogram_and_transitions(exp_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects of Initial Distribution, Convergence Diagnosis and Burn-in Time\n",
    "In the case of the Gaussian above, our `init_function`(initial distribution) was $\\mathcal{N}(60,1)$ which is exactly the same as the distribution we were trying to sample, i.e, we started the chain from the stationary distribution. However in general, we obviously don't have the ability to sample from the distribution we were trying to sample from in the first place! Notice that in the exponential, it goes down drastically from 10 where we started the chain, and oscillates more around lower values.\n",
    "\n",
    "Now run the following code. As you run it, think about the following questions - Are there some samples we need to ignore at the beginning? Explain what is happening with the Markov Chain based on the parameters we've used and your observations from the state vs iteration plot and tell us approximately how many samples we need to ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prior = lambda: np.random.normal(loc=1000)\n",
    "sample_candidate = lambda theta: np.random.normal(loc=theta, scale=3)\n",
    "normal_pdf = utils.normal(60,30)\n",
    "scorer = lambda x, y: normal_pdf(y)/normal_pdf(x)\n",
    "normal_samples = metropolis_hastings(sample_candidate, sample_prior, scorer, 5000, 1)\n",
    "utils.plot_transitions(normal_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prior = lambda: np.random.normal(loc=75)\n",
    "sample_candidate = lambda theta: np.random.normal(loc=theta, scale=0.1)\n",
    "normal_pdf = utils.normal(60,1)\n",
    "scorer = lambda x, y: normal_pdf(y)/normal_pdf(x)\n",
    "normal_samples = metropolis_hastings(sample_candidate, sample_prior, scorer, 5000, 1)\n",
    "utils.plot_transitions(normal_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks of MCMC Techniques\n",
    "\n",
    "Now we'll evaluate the effectiveness of our sampling technique on a variety of models. The examples below will highlight some of the drawbacks and idiosyncrasies of MCMC techniques. We will look at this in the context of distributions with two peaks (two separated regions of high probability), also known as bimodal distributions. We will see that the peaks may not be sampled appropriately.\n",
    "\n",
    "## Bimodal Mixture of Gaussians\n",
    "\n",
    "A mixture of Gaussians is obtained when you have two subpopulations ('classes') each distributed normally($\\mathcal{N}(\\mu_1,\\sigma_1^2)$ and $\\mathcal{N}(\\mu_2,\\sigma_2^2)$) . An example is heights of people with the subclasses of men and women. In the mixture model the 'classes' have probabilities $p$ and $1-p$ respectively. So the pdf of this distribution would be $$p\\cdot f_X(x\\ |\\text{ class 1}) + (1-p)\\cdot f_X(x\\ |\\text{ class 2}) = p\\cdot \\frac{1}{\\sqrt{2\\pi \\sigma_1^2}} e^{-\\frac{(x - \\mu_1)^2}{2\\sigma_1^2}} + (1-p)\\cdot \\frac{1}{\\sqrt{2\\pi \\sigma_2^2}} e^{-\\frac{(x - \\mu_2)^2}{2\\sigma_2^2}}\n",
    "$$\n",
    "\n",
    "For there to be two peaks in the pdf (to be bimodal), there should be (loosely speaking) sufficient separation between the means with respect to the standard deviations(the widths of the distributions). Otherwise if the peaks are too close relative to the widths, it is possible for the mixture to lead to just one central peak between the two means. There are exact conditions for this you can look up if you are interested. \n",
    "\n",
    "For this part we will be using a mixture with equal probabilities $(0.5)$ on each of the individual Gaussians with means $60$ and $40$. Try MH on this distribution for standard deviations of $5,3,1$ for each of the individual Gaussians. You should see that one of the peaks dominates (could be either one) as the standard deviation reduces even though both classes have an equal probability. For low std devs 2 and 1, only one peak should show up. What effect do you think changing the standard deviation has? What possible disadvantage of MH does this bimodal distribution show?\n",
    "\n",
    "*Hints:* \n",
    "* *There should be jumps in your transition plots (at least for std devs 4 and 5). Think about what these jumps mean.*\n",
    "* *As the widths of the peaks grow thinner, how often would you propose a state on the other peak that has a high probability?*\n",
    "* *How many iterations would we need to tell that we've got samples from both classes a reasonable number of times?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_bimodal(stdev):\n",
    "    \"\"\"Samples from bimodal mixture of Gaussians\n",
    "    with standard deviation stdev, as described above.\"\"\"\n",
    "    \n",
    "    pdf = utils.gauss_mix(0.5,40,stdev,60,stdev)\n",
    "\n",
    "    sample_candidate = lambda theta: np.random.normal(loc=theta)\n",
    "    new_scorer = lambda x,y: pdf(y)/pdf(x) \n",
    "    new_prior = lambda : 50\n",
    "\n",
    "    points = metropolis_hastings(sample_candidate,new_prior,new_scorer,10000)\n",
    "    utils.plot_histogram_and_transitions(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_from_bimodal(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_from_bimodal(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_from_bimodal(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Secret Messages Using MCMC\n",
    "\n",
    "Now we'll use our algorithm to solve a mystery on the EE 126 staff. Wacky Will and Jumpy Justin are sending each other secret messages using a cipher: each character in the message is either an uppercase letter or a space (denoted _ ) (27 possible characters), and their cipher is a one-to-one mapping between the letters of the alphabet + _. To send a message, they replace each character with the corresponding character in their cipher.\n",
    "\n",
    "For example, if their cipher was the following:\n",
    "\n",
    "A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X | Y | Z | _\n",
    "- | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | -\n",
    "X | T | A | G | M | L | C | Q | P | Z | H | W | J | I | E | B | K | O | _ | F | D | U | L | N | R | S | Y\n",
    "\n",
    "Then 'HELLO WORLD' would translate to 'QMWWEYLEOWG'. Your job is to decode their message using the Metropolis-Hastings algorithm you wrote above. Our goal is to find the cipher that maximizes the likelihood of seeing the characters in the translated message. The cipher will be a list of integers representing the letter of the alphabet that the letter corresponding to that index should be translated to. For example, if \"g\" should be replaced with \"a\", then $\\text{cipher}[6]$ should equal 0. **Note: 0-index when counting letters of the alphabet.**\n",
    "\n",
    "Our model of language will consider each character to be dependent only on the previous character. For example,\n",
    "\n",
    "$$\\mathbb{P}(x_1 = c, x_2 = a, x_3 = t) = \\mathbb{P}(x_1 = c)\\mathbb{P}(x_2 = a|x_1 = c)\\mathbb{P}(x_3 = t|x_2 = a)$$\n",
    "\n",
    "These transition probabilities will be calculated empirically by counting the number of transitions between every pair of characters in a large corpus of text.\n",
    "\n",
    "The state space is the set of all ciphers $X = \\{\\sigma : \\sigma \\text{ is a permutation of the English alphabet and '  '}\\}$. $|X| = 27!$, so finding the most likely cipher is far too costly to calculate naively, but we can sample from the space of all ciphers intelligently by using Metropolis-Hastings with the following functions:\n",
    "\n",
    "**Proposals**: To propose new ciphers, we will randomly swap two characters in our cipher.\n",
    "\n",
    "**Acceptance Function**: Note that because our proposal distribution is symmetric, the acceptance probability becomes $A(x,y) = min\\{1, \\frac{f(y)}{f(x)}\\}$. \n",
    "$f(x)$ is the probability of observing the sequence of characters in the message decoded by cipher $x$: \n",
    "\n",
    "$$f(\\cdot) = \\mathbb{P}(x_1 = \\text{letter}_1)\\mathbb{P}(x_2 = \\text{letter}_2|x_1 = \\text{letter}_1)\\mathbb{P}(x_3 = \\text{letter}_3|x_2 = \\text{letter}_2)$$\n",
    "\n",
    "$f(\\cdot)$ is _not_ a valid probability over all ciphers because we don't normalize, but it is sufficient for us to compare two ciphers.\n",
    "\n",
    "Here is an example of one iteration of the algorithm. If we are dealing with a reduced alphabet of $\\{A,B,C,D,\\text{' '}\\}$ and our current cipher is $[ 2,0,4,3,1 ]$, then we are mapping $A->C, B->A, C->\\text{' '}$, etc. If our proposal function suggests the perturbed cipher $[ 4,0,2,3,1 ]$, we will accept this cipher as our new state with probability $min \\{1, \\frac{f([ 4,0,2,3,1 ])}{f([ 2,0,4,3,1 ])}\\}$.\n",
    "\n",
    "We wrote functions to find the bigram frequency matrix, which gives the transition probabilities between characters, and to convert messages into a numerical format. To run the starter code below, you will need to run the `download_war_and_peace.sh` script to download corpus from which we will learn the transition probabilities. Run `./download_war_and_peace.sh` from this directory.\n",
    "\n",
    "Some final notes and tips:\n",
    "- For simplicity's sake, don't worry about the initial $P(x_1 = \\text{letter}_1)$: the sequence is 538 characters long, so this initial probability won't affect the relative probability between 2 ciphers by any noticeable amount.\n",
    "- To translate from letters to numbers quickly, take a look at the built-in `ord` function. Keep in mind that we are only working with uppercase letters, so it will map each letter to an integer in the range 65 to 90.\n",
    "- For numerical stability, to find $\\frac{f(y)}{f(x)}$, compute $\\log(f(y)) - \\log(f(x)$, and then pass it to the exp function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"war_and_peace.txt\"\n",
    "\n",
    "def build_bigram_freq_matrix(input_file):\n",
    "    \"\"\"\n",
    "    Builds a matrix that represents the transitional\n",
    "    probabilities between letters in input_file.\n",
    "    \n",
    "    bigram_freq_matrix[0][1] is the probability of\n",
    "    transitioning from the 0th letter of the alphabet\n",
    "    to the 1st letter of the alphabet, where letters\n",
    "    are zero-indexed. ' ' (space) is denoted as the\n",
    "    26th letter of the alphabet.\n",
    "    \"\"\"\n",
    "    counts = np.ones([27, 27])\n",
    "    with open(input_file, 'r', encoding='utf8') as f:\n",
    "        for _ in range(100000):\n",
    "            line = f.readline()\n",
    "            if len(line) > 2:\n",
    "                for i in range(len(line) - 2):\n",
    "                    first_char = ord(line[i].upper()) - 65 if line[i].isalpha() else 26\n",
    "                    second_char = ord(line[i+1].upper()) - 65 if line[i+1].isalpha() else 26\n",
    "                    if not (first_char == 26 and second_char == 26) and first_char <= 26 and second_char <= 26:\n",
    "                        counts[first_char][second_char] += 1\n",
    "        bigram_freq_matrix = (counts.T / np.sum(counts, axis=1)).T\n",
    "    return bigram_freq_matrix\n",
    "\n",
    "def decode(string, ordering):\n",
    "    \"\"\"\n",
    "    Decodes a string according to the given\n",
    "    ordering.\n",
    "    \n",
    "    ordering: a list representing the cipher.\n",
    "        For example, if in our cipher, 'a'\n",
    "        should be replaced with 'c', then \n",
    "        ordering[0] should equal 2.\n",
    "    \"\"\"\n",
    "    output_str = \"\"\n",
    "    for i in string:\n",
    "        first_char = ord(i.upper()) - 65 if i.isalpha() else 26\n",
    "        output_str += chr(ordering[first_char] + 65) if ordering[first_char] != 26 else \" \"\n",
    "    return output_str\n",
    "\n",
    "bigram_freq_matrix = build_bigram_freq_matrix(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Bigram Frequency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(bigram_freq_matrix, cmap='binary', interpolation='none')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starting_state():\n",
    "    \"\"\"\n",
    "    Start with a random permutation.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "\n",
    "def sample_candidate(sample):\n",
    "    \"\"\"\n",
    "    To search for new ciphers, randomly\n",
    "    swap two letters in the previous cipher.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "\n",
    "def make_acceptance_scorer(decode_string, transition_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the acceptance probability, which is the\n",
    "    probability of observing the message translated by\n",
    "    the proposed cipher devided by the probability of\n",
    "    obseving the message translated by the current\n",
    "    cipher.\n",
    "    \"\"\"\n",
    "    def scorer(current_sample, candidate):\n",
    "        # TODO\n",
    "    return scorer\n",
    "\n",
    "scorer = make_acceptance_scorer(the_secret_message, bigram_freq_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = metropolis_hastings(sample_candidate, starting_state, scorer, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch your Decoding Improve\n",
    "\n",
    "We print out the first few samples below. As you continue to sample from the space of all ciphers, the quality of your decoding should improve roughly. You may have to run the algorithm a few times to achieve good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in samples[:5]:\n",
    "    print(decode(the_secret_message, sample), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Get Sleuthy\n",
    "\n",
    "What did Will's secret message to Justin say?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = [\" \".join([str(x) for x in s]) for s in samples]\n",
    "best_combined = max(set(combined),key=combined.count)\n",
    "your_best_cipher = [int(x) for x in best_combined.split()]\n",
    "decode(the_secret_message,your_best_cipher).upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do you recognize the message?\n",
    "\n",
    "You may notice that sometimes when you run the algorithm, certain letters are not decoded correctly. For example, \"cliques\" may be translated as \"clizues.\" Why do you think that is?\n",
    "\n",
    "## YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://people.eecs.berkeley.edu/~sinclair/cs294/n1.pdf\n",
    "\n",
    "[2] http://www.mit.edu/~ilkery/papers/MetropolisHastingsSampling.pdf\n",
    "\n",
    "[3] http://statweb.stanford.edu/~cgates/PERSI/papers/MCMCRev.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
